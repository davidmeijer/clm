{"version": null, "code": "gASVPQIAAAAAAAAoQxJ0AGQBfA58EWQCjQMBAGQAUwCUKIwFaW5wdXSUjAZvdXRwdXSUjAZwYXJhbXOUjAl3aWxkY2FyZHOUjAd0aHJlYWRzlIwJcmVzb3VyY2VzlIwDbG9nlIwHdmVyc2lvbpSMBHJ1bGWUjAljb25kYV9lbnaUjA1jb250YWluZXJfaW1nlIwQc2luZ3VsYXJpdHlfYXJnc5SMD3VzZV9zaW5ndWxhcml0eZSMC2Vudl9tb2R1bGVzlIwMYmVuY2hfcmVjb3JklIwFam9iaWSUjAhpc19zaGVsbJSMD2JlbmNoX2l0ZXJhdGlvbpSMD2NsZWFudXBfc2NyaXB0c5SMCnNoYWRvd19kaXKUjA1lZGl0X25vdGVib29rlIwPY29uZGFfYmFzZV9wYXRolIwHYmFzZWRpcpSMGHJ1bnRpbWVfc291cmNlY2FjaGVfcGF0aJSMGF9faXNfc25ha2VtYWtlX3J1bGVfZnVuY5R0lF2UKE6MsmNsbSBwbG90IG5uX3RjX2V2ZXJfdl9uZXZlciAtLXNlZWQge2NvbmZpZ1tyYW5kb21fc2VlZF19IC0tb3V0Y29tZV9maWxlcyB7aW5wdXQubm5fdGNfZmlsZX0gLS1yYW5rX2ZpbGVzIHtpbnB1dC5yYW5rX2ZpbGVzfSAtLXJhbmtzX2ZpbGUge2lucHV0LnJhbmtzX2ZpbGV9IC0tb3V0cHV0X2RpciB7b3V0cHV0fSCUaA9oEoaUZYwFc2hlbGyUhZR0lC4=", "rule": "plot_nn_tc_ever_v_never", "input": ["data/0/prior/structural_prior/LOTUS_truncated_SMILES_0_CV_ranks_structure.csv.gz", "data/0/prior/structural_prior/LOTUS_truncated_SMILES_1_CV_ranks_structure.csv.gz", "data/0/prior/structural_prior/LOTUS_truncated_SMILES_2_CV_ranks_structure.csv.gz", "data/0/prior/structural_prior/LOTUS_truncated_SMILES_min1_all_freq-avg_CV_ranks_structure.csv.gz", "data/model_evaluation/0/LOTUS_truncated_SMILES_0_nn_tc_ever_v_never.csv.gz", "data/model_evaluation/0/LOTUS_truncated_SMILES_1_nn_tc_ever_v_never.csv.gz", "data/model_evaluation/0/LOTUS_truncated_SMILES_2_nn_tc_ever_v_never.csv.gz"], "log": [], "params": [], "shellcmd": "clm plot nn_tc_ever_v_never --seed 5831 --outcome_files data/model_evaluation/0/LOTUS_truncated_SMILES_0_nn_tc_ever_v_never.csv.gz data/model_evaluation/0/LOTUS_truncated_SMILES_1_nn_tc_ever_v_never.csv.gz data/model_evaluation/0/LOTUS_truncated_SMILES_2_nn_tc_ever_v_never.csv.gz --rank_files data/0/prior/structural_prior/LOTUS_truncated_SMILES_0_CV_ranks_structure.csv.gz data/0/prior/structural_prior/LOTUS_truncated_SMILES_1_CV_ranks_structure.csv.gz data/0/prior/structural_prior/LOTUS_truncated_SMILES_2_CV_ranks_structure.csv.gz --ranks_file data/0/prior/structural_prior/LOTUS_truncated_SMILES_min1_all_freq-avg_CV_ranks_structure.csv.gz --output_dir data/model_evaluation/plot/0/nn_tc_ever_v_never ", "incomplete": false, "starttime": 1750268193.5688422, "endtime": 1750268196.082305, "job_hash": 276362138, "conda_env": null, "container_img_url": null, "input_checksums": {"data/model_evaluation/0/LOTUS_truncated_SMILES_0_nn_tc_ever_v_never.csv.gz": "d8a25cbf1f461388c02eadcd2ff619391b020facdc68ce2e0e5116fa9da0ed48", "data/model_evaluation/0/LOTUS_truncated_SMILES_1_nn_tc_ever_v_never.csv.gz": "5a818cfcf845f007cefe0b7fbfb304d941468ba65e4dfc93d93da4a5edd409db", "data/model_evaluation/0/LOTUS_truncated_SMILES_2_nn_tc_ever_v_never.csv.gz": "07cfc0e59528a97eeb6da9c4b50e7573a5b0deba487103a7e1a31f9384dd96ba", "data/0/prior/structural_prior/LOTUS_truncated_SMILES_0_CV_ranks_structure.csv.gz": "10bf256d7b7f2a58b6c5ac7e5b89b1c353c525a9327921ea1ad045e8053df883", "data/0/prior/structural_prior/LOTUS_truncated_SMILES_1_CV_ranks_structure.csv.gz": "98e13b896de4552deec0a1d3bd4d7d5c68c8eb8bc813456ae99e7485a4b26436", "data/0/prior/structural_prior/LOTUS_truncated_SMILES_2_CV_ranks_structure.csv.gz": "e80d4bfc9fbc81c964a89933bb89104950113e52a1c6507bc4938c609a09901f"}}